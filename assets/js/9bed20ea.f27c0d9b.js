"use strict";(self.webpackChunkdocsite_poc_github_io=self.webpackChunkdocsite_poc_github_io||[]).push([[37106],{3905:(e,t,n)=>{n.d(t,{Zo:()=>p,kt:()=>h});var o=n(67294);function r(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function a(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);t&&(o=o.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,o)}return n}function i(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?a(Object(n),!0).forEach((function(t){r(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):a(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,o,r=function(e,t){if(null==e)return{};var n,o,r={},a=Object.keys(e);for(o=0;o<a.length;o++)n=a[o],t.indexOf(n)>=0||(r[n]=e[n]);return r}(e,t);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);for(o=0;o<a.length;o++)n=a[o],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(r[n]=e[n])}return r}var l=o.createContext({}),c=function(e){var t=o.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):i(i({},t),e)),n},p=function(e){var t=c(e.components);return o.createElement(l.Provider,{value:t},e.children)},u={inlineCode:"code",wrapper:function(e){var t=e.children;return o.createElement(o.Fragment,{},t)}},d=o.forwardRef((function(e,t){var n=e.components,r=e.mdxType,a=e.originalType,l=e.parentName,p=s(e,["components","mdxType","originalType","parentName"]),d=c(n),h=r,m=d["".concat(l,".").concat(h)]||d[h]||u[h]||a;return n?o.createElement(m,i(i({ref:t},p),{},{components:n})):o.createElement(m,i({ref:t},p))}));function h(e,t){var n=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var a=n.length,i=new Array(a);i[0]=d;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s.mdxType="string"==typeof e?e:r,i[1]=s;for(var c=2;c<a;c++)i[c]=n[c];return o.createElement.apply(null,i)}return o.createElement.apply(null,n)}d.displayName="MDXCreateElement"},2093:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>i,default:()=>u,frontMatter:()=>a,metadata:()=>s,toc:()=>c});var o=n(87462),r=(n(67294),n(3905));const a={title:"Common configuration",date:"2021-03-26",sidebar_position:1e3},i=void 0,s={unversionedId:"pipeline-components-and-applications/loaders-storage-targets/emr-etl-runner/common-configuration/index",id:"pipeline-components-and-applications/loaders-storage-targets/emr-etl-runner/common-configuration/index",title:"Common configuration",description:"Overview",source:"@site/docs/pipeline-components-and-applications/loaders-storage-targets/emr-etl-runner/common-configuration/index.md",sourceDirName:"pipeline-components-and-applications/loaders-storage-targets/emr-etl-runner/common-configuration",slug:"/pipeline-components-and-applications/loaders-storage-targets/emr-etl-runner/common-configuration/",permalink:"/docs/pipeline-components-and-applications/loaders-storage-targets/emr-etl-runner/common-configuration/",draft:!1,editUrl:"https://github.com/snowplow/snowplow.github.io/tree/main/docs/pipeline-components-and-applications/loaders-storage-targets/emr-etl-runner/common-configuration/index.md",tags:[],version:"current",lastUpdatedAt:1662725915,formattedLastUpdatedAt:"Sep 9, 2022",sidebarPosition:1e3,frontMatter:{title:"Common configuration",date:"2021-03-26",sidebar_position:1e3},sidebar:"defaultSidebar",previous:{title:"EmrEtlRunner",permalink:"/docs/pipeline-components-and-applications/loaders-storage-targets/emr-etl-runner/"},next:{title:"Iglu",permalink:"/docs/pipeline-components-and-applications/iglu/"}},l={},c=[{value:"Overview",id:"overview",level:3},{value:"Using environment variables",id:"using-environment-variables",level:3},{value:"Example configuration",id:"example-configuration",level:3},{value:"aws",id:"aws",level:3},{value:"Credentials",id:"credentials",level:4},{value:"s3",id:"s3",level:4},{value:"emr",id:"emr",level:4},{value:"collectors",id:"collectors",level:3},{value:"enrich",id:"enrich",level:3},{value:"storage",id:"storage",level:3},{value:"versions",id:"versions",level:4},{value:"monitoring",id:"monitoring",level:3},{value:"snowplow",id:"snowplow",level:4}],p={toc:c};function u(e){let{components:t,...n}=e;return(0,r.kt)("wrapper",(0,o.Z)({},p,n,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("h3",{id:"overview"},"Overview"),(0,r.kt)("p",null,"This page describes the format for the YAML file which is used to configure the EmrEtlRunner."),(0,r.kt)("h3",{id:"using-environment-variables"},"Using environment variables"),(0,r.kt)("p",null,'You can use environment variables rather than hardcoding strings in the configuration file. For example, load your AWS access key from an environment variable named "AWS',"_","SNOWPLOW","_","SECRET","_",'KEY":'),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"secret_access_key: <%= ENV['AWS_SNOWPLOW_SECRET_KEY'] %>\n")),(0,r.kt)("h3",{id:"example-configuration"},"Example configuration"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"aws:\n  # Credentials can be hardcoded or set in environment variables\n  access_key_id: <%= ENV['AWS_SNOWPLOW_ACCESS_KEY'] %>\n  secret_access_key: <%= ENV['AWS_SNOWPLOW_SECRET_KEY'] %>\n  s3:\n    region: ADD HERE\n    buckets:\n      assets: s3://snowplow-hosted-assets # DO NOT CHANGE unless you are hosting the jarfiles etc yourself in your own bucket\n      jsonpath_assets: # If you have defined your own JSON Schemas, add the s3:// path to your own JSON Path files in your own bucket here\n      log: ADD HERE\n      raw:\n        in:                  # Multiple in buckets are permitted\n          - ADD HERE          # e.g. s3://my-in-bucket\n          - ADD HERE\n        processing: ADD HERE\n        archive: ADD HERE    # e.g. s3://my-archive-bucket/in\n      enriched:\n        good: ADD HERE       # e.g. s3://my-out-bucket/enriched/good\n        bad: ADD HERE        # e.g. s3://my-out-bucket/enriched/bad\n        errors: ADD HERE     # Leave blank unless continue_on_unexpected_error: set to true below\n        archive: ADD HERE    # Where to archive enriched events to, e.g. s3://my-archive-bucket/enriched\n      shredded:\n        good: ADD HERE       # e.g. s3://my-out-bucket/shredded/good\n        bad: ADD HERE        # e.g. s3://my-out-bucket/shredded/bad\n        errors: ADD HERE     # Leave blank unless continue_on_unexpected_error: set to true below\n        archive: ADD HERE    # Where to archive shredded events to, e.g. s3://my-archive-bucket/shredded\n  emr:\n    job_name: Snowplow ETL # Give your job a name\n    ami_version: 5.9.0      # Don't change this\n    region: ADD HERE        # Always set this\n    jobflow_role: EMR_EC2_DefaultRole # Created using $ aws emr create-default-roles\n    service_role: EMR_DefaultRole     # Created using $ aws emr create-default-roles\n    placement: ADD HERE     # Set this if not running in VPC. Leave blank otherwise\n    ec2_subnet_id: ADD HERE # Set this if running in VPC. Leave blank otherwise\n    ec2_key_name: ADD HERE\n    bootstrap: []           # Set this to specify custom boostrap actions. Leave empty otherwise\n    software:\n      hbase:                # Optional. To launch on cluster, provide version, \"0.92.0\", keep quotes. Leave empty otherwise.\n      lingual:              # Optional. To launch on cluster, provide version, \"1.1\", keep quotes. Leave empty otherwise.\n    # Adjust your Spark cluster below\n    jobflow:\n      master_instance_type: m1.medium\n      core_instance_count: 2\n      core_instance_type: m1.medium\n      core_instance_ebs:    # Optional. Attach an EBS volume to each core instance.\n        volume_size: 100    # Gigabytes\n        volume_type: \"gp2\"\n        volume_iops: 400    # Optional. Will only be used if volume_type is \"io1\"\n        ebs_optimized: false # Optional. Will default to true\n      task_instance_count: 0 # Increase to use spot instances\n      task_instance_type: m1.medium\n      task_instance_bid: 0.015 # In USD. Adjust bid, or leave blank for non-spot-priced (i.e. on-demand) task instances\n    bootstrap_failure_tries: 3 # Number of times to attempt the job in the event of bootstrap failures\n    additional_info:        # Optional JSON string for selecting additional features\ncollectors:\n  format: cloudfront # Or 'clj-tomcat' for the Clojure Collector, or 'thrift' for Thrift records, or 'tsv/com.amazon.aws.cloudfront/wd_access_log' for Cloudfront access logs\nenrich:\n  versions:\n    spark_enrich: 1.10.0 # Version of the Spark Enrichment process\n  continue_on_unexpected_error: false # Set to 'true' (and set out_errors: above) if you don't want any exceptions thrown from ETL\n  output_compression: NONE # Compression only supported with Redshift, set to NONE if you have Postgres targets. Allowed formats: NONE, GZIP\nstorage:\n  versions:\n    rdb_shredder: 0.13.0        # Version of the Relational Database Shredding process\n    rdb_loader: 0.14.0          # Version of the Relational Database Loader app\n    hadoop_elasticsearch: 0.1.0 # Version of the Hadoop to Elasticsearch copying process\nmonitoring:\n  tags: {} # Name-value pairs describing this job\n  logging:\n    level: DEBUG # You can optionally switch to INFO for production\n  snowplow:\n    method: get\n    app_id: ADD HERE # e.g. snowplow\n    collector: ADD HERE # e.g. my-collector.cloudfront.net\n")),(0,r.kt)("h3",{id:"aws"},"aws"),(0,r.kt)("h4",{id:"credentials"},"Credentials"),(0,r.kt)("p",null,"The ",(0,r.kt)("inlineCode",{parentName:"p"},"access_key_id")," and ",(0,r.kt)("inlineCode",{parentName:"p"},"secret_access_key")," variables should be self-explanatory - enter your AWS access key and secret here."),(0,r.kt)("h4",{id:"s3"},"s3"),(0,r.kt)("p",null,"The ",(0,r.kt)("inlineCode",{parentName:"p"},"region"),' variable should hold the AWS region in which your four data buckets (In Bucket, Processing Bucket etc) are located, e.g. "us-east-1" or "eu-west-1". Please note that Redshift can only load data from S3 buckets located in the same region as the Redshift instance, and Amazon has not to date launched Redshift in ',(0,r.kt)("em",{parentName:"p"},"every")," region. So make sure that if you're using Redshift, the bucket specified here is in a region that supports Redshift."),(0,r.kt)("p",null,"Within the ",(0,r.kt)("inlineCode",{parentName:"p"},"s3")," section, the ",(0,r.kt)("inlineCode",{parentName:"p"},"buckets")," variables are as follows:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"assets:")," holds the ETL job's static assets (HiveQL script plus Hive deserializer). You can leave this as-is (pointing to Snowplow Analytics' own public bucket containing these assets (",(0,r.kt)("inlineCode",{parentName:"li"},"s3://snowplow-hosted-assets"),")) or replace this with your own private bucket containing the assets"),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"log:")," is the bucket in which Amazon EMR will record processing information for this job run, including logging any errors"),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"raw:")," is where you specify the paths through which your raw Snowplow events will flow. ",(0,r.kt)("inlineCode",{parentName:"li"},"in")," is an array of one or more buckets containing raw events. For ",(0,r.kt)("inlineCode",{parentName:"li"},"processing:"),", ",(0,r.kt)("strong",{parentName:"li"},"always include a sub-folder on this variable (see below for why)"),". ",(0,r.kt)("inlineCode",{parentName:"li"},"archive:")," is where your raw Snowplow events will be moved after they have been successfully processed by Elastic MapReduce"),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"enriched:")," is where you specify the paths through which your enriched Snowplow events will flow."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"shredded:")," is where you specify the paths through which your shredded types will flow")),(0,r.kt)("p",null,"For ",(0,r.kt)("inlineCode",{parentName:"p"},"good:"),", ",(0,r.kt)("strong",{parentName:"p"},"always include a sub-folder on this variable (see below for why)"),". If you are loading data into Redshift, the ",(0,r.kt)("inlineCode",{parentName:"p"},"good:")," specified here ",(0,r.kt)("strong",{parentName:"p"},"must")," be located in a region where Amazon has launched Redshift, because Redshift can only bulk load data from S3 that is located in the same region as the Redshift instance, and Redshift has not, to-date, been launched across all Amazon regions"),(0,r.kt)("p",null,"Each of the bucket variables must start with an S3 protocol - either ",(0,r.kt)("inlineCode",{parentName:"p"},"s3://")," or ",(0,r.kt)("inlineCode",{parentName:"p"},"s3n://"),". Each variable can include a sub-folder within the bucket as required, and a trailing slash is optional."),(0,r.kt)("p",null,"The ",(0,r.kt)("inlineCode",{parentName:"p"},"bad:")," entries will store any raw Snowplow log lines which did not pass the enrichment or JSON validation, along with their validation errors. The ",(0,r.kt)("inlineCode",{parentName:"p"},"errors:")," entries will contain any raw Snowplow log lines which caused an unexpected error, but only if you set continue","_","on","_","unexpected","_","error to true (see below)."),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Important 1:")," there is a bug in Hive on Amazon EMR where Hive dies if you attempt to read or write data to the root of an S3 bucket. ",(0,r.kt)("strong",{parentName:"p"},"Therefore always specify a sub-folder (e.g. ",(0,r.kt)("inlineCode",{parentName:"strong"},"/events/"),") for the ",(0,r.kt)("inlineCode",{parentName:"strong"},"raw:processing"),", ",(0,r.kt)("inlineCode",{parentName:"strong"},"enriched:good")," and ",(0,r.kt)("inlineCode",{parentName:"strong"},"shredded:good")," locations.")),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Important 2:")," do not put your ",(0,r.kt)("inlineCode",{parentName:"p"},"raw:processing")," inside your ",(0,r.kt)("inlineCode",{parentName:"p"},"raw:in")," bucket, or your ",(0,r.kt)("inlineCode",{parentName:"p"},"enriched:good")," inside your ",(0,r.kt)("inlineCode",{parentName:"p"},"raw:processing"),", or you will create circular references which EmrEtlRunner cannot resolve when moving files."),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Example bucket settings")),(0,r.kt)("p",null,"Here is an example configuration:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"buckets:\n  assets: s3://snowplow-hosted-assets\n  log: s3n://my-snowplow-etl/logs/\n  raw:\n    in: [s3n://my-snowplow-logs/]\n    processing: s3n://my-snowplow-etl/processing/\n    archive: s3://my-archive-bucket/raw\n  enriched:\n    good: s3://my-data-bucket/enriched/good\n    bad: s3://my-data-bucket/enriched/bad\n    errors: s3://my-data-bucket/enriched/errors\n    archive: s3://my-data-bucket/enriched/archive\n  shredded:\n    good: s3://my-data-bucket/shredded/good\n    bad: s3://my-data-bucket/shredded/bad\n    errors: s3://my-data-bucket/shredded/errors\n")),(0,r.kt)("p",null,"Please note that all buckets must exist prior to running EmrEtlRunner; trailing slashes are optional."),(0,r.kt)("h4",{id:"emr"},"emr"),(0,r.kt)("p",null,"The EmrEtlRunner makes use of Amazon Elastic Mapreduce (EMR) to process the raw log files and output the cleaned, enriched Snowplow events table."),(0,r.kt)("p",null,"This section of the config file is where we configure the operation of EMR. The variables with defaults can typically be left as-is, but you will need to set:"),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("inlineCode",{parentName:"li"},"region"),', which is the Amazon EC2 region in which the job should run, e.g. "us-east-1" or "eu-west-1"'),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("inlineCode",{parentName:"li"},"ec2_key_name"),", which is the name of the Amazon EC2 key that you",(0,r.kt)("br",{parentName:"li"}),"set up in the ","[","[","Dependencies|1-Installing-EmrEtlRunner#dependencies","]","]"," above")),(0,r.kt)("p",null,"Make sure that the EC2 key you specify belongs in the region you specify, or else EMR won't be able to find the key. ",(0,r.kt)("strong",{parentName:"p"},"It's strongly recommended that you choose the same Amazon region as your S3 buckets are located in.")),(0,r.kt)("p",null,"Since 6th April 2015, all new Elastic MapReduce users have been required to use IAM roles with EMR. You can leave the two ",(0,r.kt)("inlineCode",{parentName:"p"},"..._role")," fields as they are, however you must first create these default EMR roles using the AWS Command Line Interface (",(0,r.kt)("a",{parentName:"p",href:"http://docs.aws.amazon.com/cli/latest/userguide/installing.html"},"installation-instructions"),"), like so:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"$ aws emr create-default-roles\n")),(0,r.kt)("p",null,"Additionally, fill in ",(0,r.kt)("strong",{parentName:"p"},"one")," of these two:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"placement"),", which is the Amazon EC2 region ",(0,r.kt)("strong",{parentName:"li"},"and")," availability zone",(0,r.kt)("br",{parentName:"li"}),'in which the job should run, e.g. "us-east-1a" or "eu-west-1b"'),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"ec2_subnet_id"),", which is the ID of the Amazon EC2 subnet you want",(0,r.kt)("br",{parentName:"li"}),"to run the job in")),(0,r.kt)("p",null,"You only need to set one of these (they are mutually exclusive settings), but you must set one."),(0,r.kt)("p",null,"The ",(0,r.kt)("inlineCode",{parentName:"p"},"software:")," section lets you start up Lingual and/or HBase when you start up your Elastic MapReduce cluster. This is the configuration to start up both, specifying the versions to start:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},'software:\n  hbase: "0.92.0"\n  lingual: "1.1"\n')),(0,r.kt)("h3",{id:"collectors"},"collectors"),(0,r.kt)("p",null,"The ",(0,r.kt)("inlineCode",{parentName:"p"},"format")," field describes the format of the EmrEtlRunner's input. The options are:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},'"cloudfront" for the Cloudfront Collector'),(0,r.kt)("li",{parentName:"ul"},'"clj-tomcat" for the Clojure Collector'),(0,r.kt)("li",{parentName:"ul"},'"thrift" for Thrift raw events'),(0,r.kt)("li",{parentName:"ul"},'"tsv/com.amazon.aws.cloudfront/wd',"_","access","_",'log" for Cloudfront access logs')),(0,r.kt)("p",null,"See the ","[","[","EmrEtlRunner Input Formats","]","]"," page."),(0,r.kt)("h3",{id:"enrich"},"enrich"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"spark_enrich"),": version of the Spark Enrich jar"),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"continue_on_unexpected_error"),': continue processing even on unexpected row-level errors, e.g. an input file not matching the expected CloudFront format. Off ("false") by default')),(0,r.kt)("h3",{id:"storage"},"storage"),(0,r.kt)("h4",{id:"versions"},"versions"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"rdb_shredder"),": version of the RDB Shredder jar"),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"rdb_loader"),": version of the RDB Loader jar"),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"hadoop_elasticsearch"),": version of the Hadoop Elasticsearch Sink")),(0,r.kt)("h3",{id:"monitoring"},"monitoring"),(0,r.kt)("p",null,"This section deals with metadata around the EmrEtlRunner and RDB Loader."),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"tags"),": a dictionary of name-value pairs describing the job"),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"logging"),": how verbose/chatty the log output from EmrEtlRunner should be.")),(0,r.kt)("h4",{id:"snowplow"},"snowplow"),(0,r.kt)("p",null,"The ",(0,r.kt)("inlineCode",{parentName:"p"},"snowplow"),' section allows the ETL apps to send Snowplow events describing their own progress. To disable this internal tracking, remove the "snowplow" field from the configuration.'),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"method"),': "get" or "post"'),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"app_id"),": ID for the pipeline"),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"collector"),": Endpoint to which events should be sent")))}u.isMDXComponent=!0}}]);