"use strict";(self.webpackChunkdocsite_poc_github_io=self.webpackChunkdocsite_poc_github_io||[]).push([[63471],{3905:(e,t,a)=>{a.d(t,{Zo:()=>u,kt:()=>m});var n=a(67294);function o(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function r(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function i(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?r(Object(a),!0).forEach((function(t){o(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):r(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function s(e,t){if(null==e)return{};var a,n,o=function(e,t){if(null==e)return{};var a,n,o={},r=Object.keys(e);for(n=0;n<r.length;n++)a=r[n],t.indexOf(a)>=0||(o[a]=e[a]);return o}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(n=0;n<r.length;n++)a=r[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(o[a]=e[a])}return o}var l=n.createContext({}),d=function(e){var t=n.useContext(l),a=t;return e&&(a="function"==typeof e?e(t):i(i({},t),e)),a},u=function(e){var t=d(e.components);return n.createElement(l.Provider,{value:t},e.children)},p={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},c=n.forwardRef((function(e,t){var a=e.components,o=e.mdxType,r=e.originalType,l=e.parentName,u=s(e,["components","mdxType","originalType","parentName"]),c=d(a),m=o,h=c["".concat(l,".").concat(m)]||c[m]||p[m]||r;return a?n.createElement(h,i(i({ref:t},u),{},{components:a})):n.createElement(h,i({ref:t},u))}));function m(e,t){var a=arguments,o=t&&t.mdxType;if("string"==typeof e||o){var r=a.length,i=new Array(r);i[0]=c;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s.mdxType="string"==typeof e?e:o,i[1]=s;for(var d=2;d<r;d++)i[d]=a[d];return n.createElement.apply(null,i)}return n.createElement.apply(null,a)}c.displayName="MDXCreateElement"},3338:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>l,contentTitle:()=>i,default:()=>p,frontMatter:()=>r,metadata:()=>s,toc:()=>d});var n=a(87462),o=(a(67294),a(3905));const r={title:"Querying failed events in Athena and BigQuery",date:"2020-04-30",sidebar_position:5e3},i=void 0,s={unversionedId:"managing-data-quality/failed-events/failed-events-in-athena-and-bigquery/index",id:"managing-data-quality/failed-events/failed-events-in-athena-and-bigquery/index",title:"Querying failed events in Athena and BigQuery",description:"Athena\xa0on AWS and\xa0BigQuery\xa0on GCP are tools that let you query your failed events, using the cloud storage files as a back-end data source.",source:"@site/docs/managing-data-quality/failed-events/failed-events-in-athena-and-bigquery/index.md",sourceDirName:"managing-data-quality/failed-events/failed-events-in-athena-and-bigquery",slug:"/managing-data-quality/failed-events/failed-events-in-athena-and-bigquery/",permalink:"/docs/managing-data-quality/failed-events/failed-events-in-athena-and-bigquery/",draft:!1,editUrl:"https://github.com/snowplow/snowplow.github.io/tree/main/docs/managing-data-quality/failed-events/failed-events-in-athena-and-bigquery/index.md",tags:[],version:"current",lastUpdatedAt:1662725915,formattedLastUpdatedAt:"Sep 9, 2022",sidebarPosition:5e3,frontMatter:{title:"Querying failed events in Athena and BigQuery",date:"2020-04-30",sidebar_position:5e3},sidebar:"defaultSidebar",previous:{title:"Accessing failed event aggregates via the API",permalink:"/docs/managing-data-quality/failed-events/accessing-failed-event-aggregates-via-the-api/"},next:{title:"Event Recovery for BDP customers",permalink:"/docs/managing-data-quality/event-recovery-for-bdp-users/"}},l={},d=[{value:"Athena instructions",id:"athena-instructions",level:2},{value:"BigQuery instructions",id:"bigquery-instructions",level:2},{value:"Missing fields\xa0\u26a0\ufe0f",id:"missing-fields\ufe0f",level:5},{value:"But why bother with schemas?",id:"but-why-bother-with-schemas",level:4}],u={toc:d};function p(e){let{components:t,...r}=e;return(0,o.kt)("wrapper",(0,n.Z)({},u,r,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("p",null,(0,o.kt)("a",{parentName:"p",href:"https://aws.amazon.com/athena/"},"Athena"),"\xa0on AWS and\xa0",(0,o.kt)("a",{parentName:"p",href:"https://cloud.google.com/bigquery"},"BigQuery"),"\xa0on GCP are tools that let you query your failed events, using the cloud storage files as a back-end data source."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"SELECT data.failure.messages FROM adapter_failures\nWHERE from_iso8601_timestamp(data.failure.timestamp) > timestamp '2020-04-01'\n")),(0,o.kt)("p",null,"This is great for debugging your pipeline without the need to load your failed events into a separate database."),(0,o.kt)("p",null,"The ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/snowplow-incubator/snowplow-badrows-tables"},"snowplow-badrows-tables repo")," contains resources and instructions to set up Athena/BigQuery for your pipeline. To create tables, you need to provide a table definition corresponding to the JSON schema for each of your failed event files. Each different failed event type (e.g. schema violations, adapter failures) has its own JSON schema, and therefore requires its own separate table."),(0,o.kt)("h2",{id:"athena-instructions"},"Athena instructions"),(0,o.kt)("p",null,"Go to\xa0",(0,o.kt)("a",{parentName:"p",href:"https://eu-central-1.console.aws.amazon.com/athena/home"},"the Athena dashboard"),"\xa0and use the query editor. Start by creating a database named after your pipeline (e.g. prod1 or qa1):"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"CREATE DATABASE IF NOT EXISTS {{ DATABASE }}\n")),(0,o.kt)("p",null,"Then run each sql statement provided in the ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/snowplow-incubator/snowplow-badrows-tables/tree/master/athena"},"badrows-tables repo")," by copying them into the Athena query editor:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("img",{src:a(216).Z,width:"1362",height:"798"}))),(0,o.kt)("p",null,"Create table in Athena"),(0,o.kt)("p",null,"As example of using your Athena tables, you might start by getting counts of each failed event type from the last week. Repeat this query for each table you have created:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"SELECT COUNT(*) FROM schema_violations\nWHERE from_iso8601_timestamp(data.failure.timestamp) > DATE_ADD('day', -7, now())\n")),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("img",{src:a(55112).Z,width:"1362",height:"762"}))),(0,o.kt)("p",null,"Athena query"),(0,o.kt)("p",null,"If you have schema violations, you might want to find which tracker sent the event:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"SELECT data.payload.enriched.app_id, COUNT(*) FROM schema_violations\nWHERE from_iso8601_timestamp(data.failure.timestamp) > DATE_ADD('day', -7, now())\nGROUP BY data.payload.enriched.app_id\n")),(0,o.kt)("p",null,"You can do a deeper dive into the error messages to get a explanation of the last 10 failures:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"SELECT data.failure.messages[1].field AS field,\n       data.failure.messages[1].value AS value,\n       data.failure.messages[1].error AS error,\n       data.failure.messages[1].json AS json,\n       data.failure.messages[1].schemaKey AS schemaKey,\n       data.failure.messages[1].schemaCriterion AS schemaCriterion\nFROM schema_violations\nORDER BY data.failure.timestamp DESC\nLIMIT 10\n")),(0,o.kt)("h2",{id:"bigquery-instructions"},"BigQuery instructions"),(0,o.kt)("p",null,"These instructions make use of the\xa0",(0,o.kt)("a",{parentName:"p",href:"https://cloud.google.com/bigquery/docs/bq-command-line-tool"},"bq command-line tool"),"\xa0which is packaged with the\xa0",(0,o.kt)("a",{parentName:"p",href:"https://cloud.google.com/sdk/docs"},"google cloud sdk"),". Follow the sdk instructions for how to\xa0",(0,o.kt)("a",{parentName:"p",href:"https://cloud.google.com/sdk/docs/initializing"},"initialize and authenticate the sdk"),". Also take a look at the\xa0",(0,o.kt)("a",{parentName:"p",href:"https://console.cloud.google.com/bigquery"},"BigQuery dashboard"),"\xa0as you run these commands, so you can see your tables as you create them."),(0,o.kt)("h5",{id:"missing-fields\ufe0f"},"Missing fields\xa0\u26a0\ufe0f"),(0,o.kt)("p",null,(0,o.kt)("em",{parentName:"p"},"The following fields are missing from the bigquery table definitions:")),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("em",{parentName:"li"},"Enrichment failures:\xa0",(0,o.kt)("inlineCode",{parentName:"em"},"data.failure.message.error"))),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("em",{parentName:"li"},"Loader iglu error:\xa0",(0,o.kt)("inlineCode",{parentName:"em"},"data.failure.dataReports.targets"))),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("em",{parentName:"li"},"Loader recovery error:\xa0",(0,o.kt)("inlineCode",{parentName:"em"},"data.failure.error.location"))),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("em",{parentName:"li"},"Schema violations:\xa0",(0,o.kt)("inlineCode",{parentName:"em"},"data.failure.messages.error"))),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("em",{parentName:"li"},"Tracker protocol violations:\xa0",(0,o.kt)("inlineCode",{parentName:"em"},"data.failure.messages.error")))),(0,o.kt)("p",null,(0,o.kt)("em",{parentName:"p"},'We have omitted fields from the table definitions if they are "polymorphic", e.g. where they can be a string or an object depending on the context. Unfortunately this makes the fields inaccessible in BigQuery. This problem will be fixed in future versions of Snowplow, by removing polymorphic fields (see issues in\xa0',(0,o.kt)("a",{parentName:"em",href:"https://github.com/snowplow-incubator/snowplow-badrows/issues/50"},"snowplow-badrows"),"\xa0and\xa0",(0,o.kt)("a",{parentName:"em",href:"https://github.com/snowplow/iglu-central/issues/1075"},"iglu-central"),").")),(0,o.kt)("p",null,"Create a dataset to contain your failed event tables:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"bq mk --data_location=EU bad_rows_prod1\n# Dataset 'my-snowplow-project:bad_rows_prod1' successfully created.\n")),(0,o.kt)("p",null,"The\xa0",(0,o.kt)("inlineCode",{parentName:"p"},"--data-location"),"\xa0should match the location of your bad rows bucket. Also replace\xa0",(0,o.kt)("inlineCode",{parentName:"p"},"prod1"),"\xa0with the name of your pipeline."),(0,o.kt)("p",null,"Now run\xa0",(0,o.kt)("inlineCode",{parentName:"p"},"bq mk"),"\xa0for each table definition in the badrows-tables repo. Use the\xa0",(0,o.kt)("inlineCode",{parentName:"p"},"--external_table_definition"),"\xa0parameter so that bigquery uses the bucket as the back-end data source. Here is how to run the command for the first three tables (note you should change the dataset name\xa0",(0,o.kt)("inlineCode",{parentName:"p"},"bad_rows_prod1"),"\xa0to match the dataset you just created):"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"bq mk \\\n  --display_name=\"Adapter failures\" \\\n  --external_table_definition=./adapter_failures.json \\\n  bad_rows_prod1.adapter_failures\n\n# Table 'my-snowplow-project:bad_rows_prod1.adapter_failures' successfully created.\n\nbq mk \\\n  --display_name \"Schema violations\" \\\n  --external_table_definition=./schema_violations.json \\\n  bad_rows_prod1.schema_violations\n\n# Table 'my-snowplow-project:bad_rows_prod1.schema_violations' successfully created.\n\nbq mk \\\n  --display_name \"Tracker protocol violations\" \\\n  --external_table_definition=./tracker_protocol_violations.json \\\n  bad_rows_prod1.tracker_protocol_violations\n\n# Table 'my-snowplow-project:bad_rows_prod1.tracker_protocol_violations' successfully created.\n")),(0,o.kt)("p",null,"You can query your tables from the query editor in the\xa0",(0,o.kt)("a",{parentName:"p",href:"https://console.cloud.google.com/bigquery"},"BigQuery console"),". You might want to start by getting counts of each failed event type from the last week. This query will work, but it is relatively expensive because it will scan all files in the\xa0",(0,o.kt)("inlineCode",{parentName:"p"},"schema_violations"),"\xa0directory:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"SELECT COUNT(*) FROM bad_rows_prod1.schema_violations\nWHERE data.failure.timestamp > TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 7 DAY);\n")),(0,o.kt)("p",null,"You can construct a more economical query by using the\xa0",(0,o.kt)("inlineCode",{parentName:"p"},"_FILE_NAME"),"\xa0pseudo column to restrict the scan to files from the last week:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"SELECT COUNT(*) FROM bad_rows_prod1.schema_violations\nWHERE DATE(PARSE_TIMESTAMP('%Y-%m-%dT%H:%M:%S', LTRIM(REGEXP_EXTRACT(_FILE_NAME, 'output-[0-9]+-[0-9]+-[0-9]+T[0-9]+:[0-9]+:[0-9]+'), 'output-'))) >= DATE_SUB(CURRENT_DATE, INTERVAL 7 DAY);\n")),(0,o.kt)("p",null,"You can repeat that query for each table you created in your bad rows dataset."),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("img",{src:a(35066).Z,width:"1445",height:"847"}))),(0,o.kt)("p",null,"If you have schema violations, you might want to find which tracker sent the event:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"SELECT data.payload.enriched.app_id, COUNT(*) FROM bad_rows_prod1.schema_violations\nWHERE DATE(PARSE_TIMESTAMP('%Y-%m-%dT%H:%M:%S', LTRIM(REGEXP_EXTRACT(_FILE_NAME, 'output-[0-9]+-[0-9]+-[0-9]+T[0-9]+:[0-9]+:[0-9]+'), 'output-'))) >= DATE_SUB(CURRENT_DATE, INTERVAL 7 DAY)\nGROUP BY data.payload.enriched.app_id;\n")),(0,o.kt)("p",null,"If you have tracker protocol failures, you can do a deeper dive into the error messages to get a explanation of the last 10 failures:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"SELECT data.failure.messages[OFFSET(0)].field AS field,\n       data.failure.messages[OFFSET(0)].value AS value,\n       data.failure.messages[OFFSET(0)].expectation AS expectation,\n       data.failure.messages[OFFSET(0)].schemaKey AS schemaKey,\n       data.failure.messages[OFFSET(0)].schemaCriterion AS schemaCriterion\nFROM bad_rows_prod1.tracker_protocol_violations\nWHERE DATE(PARSE_TIMESTAMP('%Y-%m-%dT%H:%M:%S', LTRIM(REGEXP_EXTRACT(_FILE_NAME, 'output-[0-9]+-[0-9]+-[0-9]+T[0-9]+:[0-9]+:[0-9]+'), 'output-'))) >= DATE_SUB(CURRENT_DATE, INTERVAL 7 DAY)\nORDER BY data.failure.timestamp DESC\nLIMIT 10;\n")),(0,o.kt)("h4",{id:"but-why-bother-with-schemas"},"But why bother with schemas?"),(0,o.kt)("p",null,"BigQuery has a 'Auto-detect' feature to automatically generate the table definition for you by inspecting the file contents. So you might wonder why it is necessary to provide explicit schema definitions for your tables."),(0,o.kt)("p",null,"There are two potential pitfalls when using the autogenerated schema with the Snowplow bad rows files:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},'Optional fields. BigQuery might not "notice" that a field exists, depending on the sample of data used to detect the schema.'),(0,o.kt)("li",{parentName:"ul"},"Polymorphic fields, i.e. a field that can be either a string or an object. BigQuery will throw an exception if it sees an unexpected value for a field.")))}p.isMDXComponent=!0},55112:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/athena-count-3c29355bdfad7e01cc24cb3bd09105b9.png"},216:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/athena-create-table-9b949019dd6014dd9c28e45da9f37ba2.png"},35066:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/bigquery-count-06422ef48c38bef0a72bc39b1b739bf9.png"}}]);